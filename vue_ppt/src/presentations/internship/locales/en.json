{
  "presentation": {
    "title": "Kurt Yang's Google Internship",
    "description": "A glimpse into a summer engineering rotation on Google's AI Systems team",
    "author": "Kurt Yang",
    "dateLabel": "Summer 2024"
  },
  "slide1": {
    "kicker": "Internship Kickoff",
    "title": "Kurt Yang's Google Internship",
    "subtitle": "SWE Intern, Ph.D.",
    "highlights": {
      "title": "Projects",
      "items": {
        "maxtext": "MaxText Model Evaluation Tool",
        "moba": "Mixture of Experts Attention (MoBA)"
      }
    }
  },
  "slide7": {
    "kicker": "Architecture Overview",
    "title": "How the OpenAI API ties into MaxText",
    "lede": "The server routes standard OpenAI requests into MaxText batch inference, giving LM-Eval-Harness a plug-and-play backend without touching TPU code.",
    "labels": {
      "framing": "Why it works"
    },
    "client": {
      "badge": "Client",
      "title": "LM-Eval-Harness / Evalchemy",
      "summary": "Handles datasets, issues requests, and converts responses into benchmark scores."
    },
    "api": {
      "badge": "Server",
      "title": "FastAPI OpenAI-Compatible Server",
      "summary": "Applies chat templates and batches requests before forwarding them to MaxText."
    },
    "backend": {
      "badge": "Backend",
      "title": "MaxText Batch Inference",
      "summary": "Runs inference with the provided parameters and returns generations."
    },
    "footnote": "This stack lets benchmarking frameworks call MaxText like any OpenAI endpoint while keeping TPU orchestration and batching inside our server."
  },
  "slide8": {
    "kicker": "Server Pipeline",
    "title": "How the server prepares workloads",
    "lede": "Between the OpenAI request and MaxText inference, the server handles templating and batching so chat-style clients stay simple.",
    "labels": {
      "result": "Outcome"
    },
    "ingest": {
      "badge": "Ingest",
      "title": "Normalize requests",
      "summary": "Applies the tokenizer's chat template so each raw request becomes a complete prompt with BOS tokens and required formatting."
    },
    "batch": {
      "badge": "Batch",
      "title": "Group compatible prompts",
      "summary": "Chat API callers send one conversation at a time; the server batches these single requests internally."
    },
    "dispatch": {
      "badge": "Dispatch",
      "title": "Run on MaxText",
      "summary": "Batched prompts are sent to the MaxText JetStream engine, which performs decoding and returns the generations."
    },
    "footnote": "Result: chat workloads gain batching efficiency without breaking the OpenAI contract or requiring client-side template changes."
  },
  "slide9": {
    "kicker": "Core API Surface",
    "title": "The functions powering the Inference",
    "lede": "Evaluation frameworks hit two core MaxText primitives. Once those are wired, LM-Eval-Harness can fan out across benchmarks with no custom code.",
    "labels": {
      "signature": "Function signature",
      "example": "Usage example"
    },
    "generate": {
      "kicker": "Free-form",
      "title": "generate_until",
      "summary": "Runs open-ended generation using the provided sampling parameters.",
      "signature": "def generate_until(\n    prompts: List[str],\n    sampling_params: SamplingParams\n) -> List[Completion]: ...",
      "example": "results = generate_until(\n    prompts=[\"Who discovered penicillin?\", \"Summarize the safety policy\"],\n    sampling_params=SamplingParams(\n        max_tokens=128,\n        temperature=0.0,\n        top_p=0.95,\n        echo=False\n    )\n)",
      "detail": "Used for open-ended tasks (e.g., AIME, long-form generation). Sampling parameters control temperature, top_p, max_tokens, etc."
    },
    "loglikelihood": {
      "kicker": "Scoring",
      "title": "loglikelihood",
      "summary": "Returns per-token log probabilities for candidate continuations.",
      "signature": "def loglikelihood(\n    contexts: List[str],\n    continuations: List[str],\n    sampling_params: SamplingParams,\n    logprobs: int = 0\n) -> List[LogprobResult]: ...",
      "example": "scores = loglikelihood(\n    contexts=[\"The capital of France is\"],\n    continuations=[\" Paris\", \" Seattle\"],\n    sampling_params=SamplingParams(max_tokens=0, temperature=0.0),\n    logprobs=5\n)",
      "detail": "Log prob = log(P(next token | context)). Needed for multiple-choice tasks and perplexity scoring." 
    }
  },
  "slide10": {
    "kicker": "Benchmark Usage",
    "title": "How benchmarks call into MaxText",
    "lede": "Different benchmark families lean on different MaxText APIs. Here’s how loglikelihood and generate_until show up in practice.",
    "labels": {
      "howItWorks": "How it works",
      "pseudocode": "Pseudocode",
      "takeaway": "Takeaway"
    },
    "mmlu": {
      "badge": "Multiple choice",
      "title": "MMLU (loglikelihood)",
      "summary": "Scores each answer option by comparing continuation probabilities.",
      "howItWorks": "LM-Eval-Harness sends the question stem as the context and each choice (A/B/C/D) as a continuation, then picks the option with the highest log-likelihood.",
      "code": "options = [' A', ' B', ' C', ' D']\nlogprobs = loglikelihood(\n    contexts=['The Answer is'],\n    continuations=options,\n    sampling_params=SamplingParams(max_tokens=0, temperature=0.0),\n    logprobs=0\n)\npredicted = options[argmax(logprobs)]",
      "points": {
        "choiceScoring": "Supports tasks like MMLU, ARC, and HellaSwag without bespoke scoring code.",
        "reuse": "Same call also feeds Evalchemy’s math MCQ suites, keeping inference logic centralized."
      }
    },
    "aime": {
      "badge": "Open-ended math",
      "title": "AIME25 (generate_until)",
      "summary": "Generates free-form solutions that are later parsed for the final answer.",
      "howItWorks": "Evalchemy asks MaxText to write the full reasoning chain with generate_until, then applies a regex to pull the boxed answer for grading.",
      "code": "completion = generate_until(\n    prompts=['Solve for x: 2x^2 - 5x - 3 = 0.'],\n    sampling_params=SamplingParams(max_tokens=256, temperature=0.2, top_p=0.9)\n)[0]\nanswer = extract_answer(completion, pattern=r'\\\\boxed\\{(.+?)\\}')",
      "points": {
        "freeForm": "Keeps reasoning intact so reviewers can inspect intermediate steps.",
        "regex": "Regex or string match grading means no extra inference calls are needed." 
      }
    },
    "footnote": "Together these flows cover the majority of Evalchemy and LM-Eval-Harness suites without adding new MaxText APIs."
  },
  "slide15": {
    "kicker": "Key Wins",
    "title": "What shipped this summer",
    "lede": "MaxText now has breadth, speed, and verification guardrails for LLM benchmarking.",
    "achievements": {
      "coverageTitle": "189 Benchmarks supported",
      "coverageBody": "Integrated LM-Eval-Harness + Evalchemy suites and set up to inherit future upstream additions automatically.",
      "speedTitle": "Up to 30× faster evaluation",
      "speedBody": "Batching + TPU-aware scheduling collapsed runs from double-digit hours to near real time.",
      "apiTitle": "OpenAI-compatible server",
      "apiBody": "Added a reusable API surface in MaxText so any tool can plug in without custom adapters.",
      "validationTitle": "Model bring-up verified",
      "validationBody": "Confirmed Qwen3 2507 thinking models and GPT-OSS checkpoints match their AIME25 model-card scores."
    }
  },
  "slide11": {
    "kicker": "Run It",
    "title": "Start the server, then kick off the benchmark",
    "lede": "Two terminals: one for the OpenAI-compatible server, one for LM-Eval-Harness. Everything else is configuration.",
    "labels": {
      "tip": "Tip"
    },
    "step1": {
      "title": "Launch MaxText OpenAI server",
      "summary": "Loads the Qwen3-4B checkpoint and exposes the /v1 endpoints.",
      "command": "python3 -m benchmarks.api_server.maxtext_generator \\\n    MaxText/configs/base.yml \\\n    model_name=\"qwen3-4b\" \\\n    tokenizer_path=\"Qwen/Qwen3-4B\" \\\n    load_parameters_path=\"gs://kurty-maxtext-models/Qwen3-4b-base-sft-moba/kurty-qwen3-4b-sft-moba/checkpoints/4999/items\" \\\n    per_device_batch_size=1 \\\n    ici_tensor_parallelism=4 \\\n    max_prefill_predict_length=512 \\\n    max_target_length=1024 \\\n    async_checkpointing=false \\\n    scan_layers=true \\\n    attention=\"dot_product\" \\\n    return_log_prob=true \\\n    moba_naive=true \\\n    moba_chunk_size=256 \\\n    moba_topk=16",
      "points": {
        "modelConfig": "Configures the TPU mesh, tokenizer, and checkpoint path for Qwen3-4B.",
        "batchTpu": "Enables batching + log probabilities so both completions and scoring routes are ready."
      }
    },
    "step2": {
      "title": "Run LM-Eval-Harness",
      "summary": "Points the local-completions backend at the server and runs MMLU.",
      "command": "python -m eval.eval \\\n    --model local-completions \\\n    --model_args \"pretrained=Qwen/Qwen3-4B,base_url=http://localhost:8000/v1/completions,tokenizer_backend=huggingface,tokenizer=Qwen/Qwen3-4B,model=qwen3_4b\" \\\n    --tasks mmlu \\\n    --batch_size 16 \\\n    --output_path logs \\\n    --limit 10",
      "points": {
        "leverageLocal": "Swaps only the base_url when moving between MaxText and OpenAI deployments.",
        "fastIter": "Limit and batch_size flags let you dry-run tasks before full suites."
      }
    },
    "footnote": "Once the server is up, the same eval command can target Evalchemy tasks by changing --tasks to aime25, math500, and beyond."
  },
  "slide12": {
    "kicker": "Live",
    "title": "DEMO"
  },
  "slide13": {
    "kicker": "Multi-host",
    "title": "Running the server across pods",
    "lede": "Large models span multiple hosts, so only host 0 runs the OpenAI server and broadcasts prompts to peer devices.",
    "labels": {
      "diagramAlt": "Broadcast architecture for multi-host setup",
      "diagramCaption": "Host 0 handles API traffic and rebroadcasts prompts",
      "tip": "Remember"
    },
    "points": {
      "host0": {
        "title": "Host 0 only",
        "body": "Launch the FastAPI server on host 0 (or pod 0) so only one OpenAI endpoint answers requests. Other hosts skip server startup."
      },
      "portforward": {
        "title": "Port-forward to evaluation VM",
        "body": "Expose host 0's port 8000 to the machine running LM-Eval so the benchmark framework sees a single endpoint.",
        "command": "bash port_forward_xpk.sh job_name=<job_name> project=<project> zone=<zone> cluster=<cluster> [namespace=<namespace>]"
      },
      "broadcast": {
        "title": "Broadcast prompts",
        "body": "Pod 0 collects API calls and uses jax.distributed.broadcast_one_to_all so every host receives the prompt and returns partial generations."
      }
    },
    "footnote": "With the broadcast pattern, scaling from 1 to N hosts is transparent to LM-Eval — only the port-forward target changes."
  },
  "slide14": {
    "kicker": "30× Speedups",
    "title": "Batching turns hours into minutes",
    "lede": "Standardized APIs and batching collapse evaluation time across hardware tiers and shot counts.",
    "labels": {
      "existing": "Existing flow",
      "newDesign": "New design",
      "speedup": "Speedup"
    },
    "v5p8_0shot": {
      "hardware": "v5p-8",
      "title": "MMLU · 0-shot",
      "model": "llama3 8B",
      "description": "0-shot MMLU runtime on v5p-8 when running llama3 8B.",
      "existingLabel": "13 hr",
      "newLabel": "1 hr"
    },
    "v5p8_5shot": {
      "hardware": "v5p-8",
      "title": "MMLU · 5-shot",
      "model": "llama3 8B",
      "description": "Few-shot prompts multiply tokens per query; batching keeps latency flat.",
      "existingLabel": "55 hr",
      "newLabel": "1.3 hr"
    },
    "v5p64": {
      "hardware": "v5p-64",
      "title": "MMLU · 0-shot",
      "model": "Qwen3-235B-A22B",
      "description": "Full Evalchemy math suite runtime on a multi-host Qwen3 235B deployment.",
      "existingLabel": "83 hr",
      "newLabel": "11 hr"
    }
  },
  "slide2": {
    "kicker": "Project Background",
    "title": "MaxText Model Evaluation Tool",
    "project": {
      "name": "Project 01",
      "tldr": {
        "heading": "Expanding MaxText benchmarking reach",
        "body": "Integrated LM-Eval-Harness and Evalchemy, unlocking 189 benchmarks and new OpenAI-compatible batch APIs that cut evaluation time by 30×."
      }
    },
    "objective": {
      "title": "Objective",
      "body": "Design an extensible evaluation layer on MaxText that broadens coverage, keeps iteration fast, and ships concise, reusable reports."
    },
    "requirements": {
      "functional": {
        "title": "Functional Requirements",
        "items": {
          "coverage": "Support the full LM-Eval-Harness and Evalchemy benchmark suites via MaxText inference.",
          "reporting": "Produce a single summary that captures model quality across all executed benchmarks."
        }
      },
      "nonFunctional": {
        "title": "Non-functional Requirements",
        "items": {
          "performance": "Accelerate runs through sampling and batched generation to keep jobs fast.",
          "scalability": "Scale across nodes so larger models can be evaluated without manual orchestration."
        }
      }
    }
  },
  "slide3": {
    "kicker": "Problem Space & Opportunity",
    "title": "MaxText Needs a New Evaluation Layer",
    "tabs": {
      "today": {
        "label": "Current Limitations",
        "spotlight": "MMLU-only"
      },
      "requirements": {
        "label": "Requirements",
        "spotlight": "Extend & Accelerate"
      }
    },
    "today": {
      "title": "Today in Production",
      "items": {
        "singleBenchmark": "Single benchmark (MMLU) coverage leaves gaps in math, code, safety, and reasoning tasks reviewers care about.",
        "singleThreaded": "One-prompt-at-a-time decoding stretches runs to hours, blocking fast iteration and larger model exploration."
      }
    },
    "requirements": {
      "title": "Requirements to Close the Gap",
      "functional": {
        "title": "Functional",
        "items": {
          "coverage": "Extend benchmark coverage beyond MMLU using standardized suites.",
          "reporting": "Ship unified quality summaries reviewers can trust."
        }
      },
      "nonFunctional": {
        "title": "Non-functional",
        "items": {
          "performance": "Batch and sample to keep jobs fast even on large checkpoints.",
          "scalability": "Support multi-node execution so bigger models aren’t bottlenecked."
        }
      }
    }
  },
  "slide4": {
    "kicker": "Design Decision",
    "title": "Build bespoke scripts or adopt frameworks?",
    "tabs": {
      "individual": {
        "label": "Individual Scripts",
        "spotlight": "Manual"
      },
      "frameworks": {
        "label": "Use Frameworks",
        "spotlight": "Standardized"
      },
      "decision": {
        "label": "Final Decision",
        "spotlight": "Rollout"
      }
    },
    "individual": {
      "title": "Individual Benchmarking Scripts",
      "summary": "Standalone scripts ship quickly but fall apart when scaling coverage.",
      "prosTitle": "Pros",
      "consTitle": "Cons",
      "pros": {
        "fastStart": "Fast to start with minimal setup or integration.",
        "noDependency": "No external dependencies — fully self-contained."
      },
      "cons": {
        "limitedCoverage": "Limited coverage; every new benchmark requires bespoke code.",
        "maintenance": "No shared automation, so maintenance cost grows with each suite."
      }
    },
    "frameworks": {
      "title": "Adopt Evaluation Frameworks",
      "detail": {
        "prosTitle": "Pros",
        "consTitle": "Cons",
        "recommendationLabel": "Recommendation"
      }
    },
    "frameworkTabs": {
      "huggingface": {
        "label": "HuggingFace Evaluate",
        "tag": "Not suited",
        "description": "General-purpose metrics package that requires manual orchestration for benchmarks.",
        "pros": {
          "customMetrics": "Good for bespoke metric plugins when ground truth is available."
        },
        "cons": {
          "manual": "Needs manual wiring of prompts, outputs, and references for every task.",
          "automation": "No automation for multi-metric logprob workflows."
        },
        "recommendation": "Skip for MaxText benchmarking — automation gaps block large-scale runs. (❌)"
      },
      "vertex": {
        "label": "Vertex AI Evaluation",
        "tag": "External dependency",
        "description": "Cloud-hosted evaluation with LLM-as-a-judge capabilities on Google Cloud.",
        "pros": {
          "judger": "Provides judgment-based evaluation for subjective tasks."
        },
        "cons": {
          "inhouse": "Functionality can be implemented in-house once core APIs exist.",
          "dependency": "Adds an external platform to configure, operate, and pay for."
        },
        "recommendation": "Not preferred — adds dependency without standardized benchmark coverage. (❌)"
      },
      "lmEval": {
        "label": "LM-Eval-Harness",
        "tag": "Standard",
        "description": "Open-source benchmark harness used across research and industry for LLM evaluation.",
        "pros": {
          "coverage": "Covers a wide catalog of standardized tasks reviewers already expect.",
          "ecosystem": "Actively maintained with first-class backend integrations."
        },
        "cons": {
          "integration": "Requires MaxText backend integration to expose generation and logprob APIs."
        },
        "recommendation": "Adopt as the primary harness for MaxText evaluations. (✅)"
      },
      "evalchemy": {
        "label": "Evalchemy",
        "tag": "Extends coverage",
        "description": "Extension on top of LM-Eval-Harness that adds AIME24/25, MATH500, and other specialized suites.",
        "pros": {
          "math": "Adds competitive math and code benchmarks reviewers asked for.",
          "extension": "Inherits LM-Eval integrations once the base harness is wired up."
        },
        "cons": {
          "dependency": "Depends on LM-Eval-Harness integration being in place."
        },
        "recommendation": "Pair with LM-Eval-Harness to cover advanced math and coding tasks. (✅)"
      }
    },
    "decision": {
      "title": "Final Decision & Rollout Plan",
      "calloutLabel": "Decision",
      "callout": "Move forward with LM-Eval-Harness plus Evalchemy."
    }
  },
  "slide5": {
    "kicker": "Architecture Choice",
    "title": "How to fill the gap between evaluation framework and MaxText",
    "lede": "Two integration paths close the gap between LM-Eval-Harness and MaxText — we evaluated both to decide how to deliver coverage fast.",
    "viewToggle": {
      "comparison": "Comparison",
      "decision": "Decision"
    },
    "native": {
      "title": "Native LM-Eval Integration",
      "summary": "Implement a MaxTextModel class directly inside LM-Eval-Harness, mirroring vLLM and llama.cpp.",
      "consTitle": "Risks",
      "cons": {
        "externalDependency": "Upstream PR reviews and release schedules gate when improvements land in production.",
        "runtimeVariance": "Couples MaxText to LM-Eval's dependency stack, creating maintenance friction across TPU environments."
      }
    },
    "server": {
      "title": "OpenAI-Compatible Server API",
      "summary": "Expose generate/loglikelihood endpoints through a lightweight MaxText server in the OpenAI format.",
      "prosTitle": "Why it wins",
      "pros": {
        "decoupling": "Decouples evaluation harness upgrades from MaxText releases so each team can move independently.",
        "standardInterface": "Delivers the industry-standard OpenAI contract already supported by LM-Eval and Evalchemy.",
        "batchAcceleration": "Supports batched decoding, cutting evaluation runtime by roughly 30× versus single-request scripts."
      }
    },
    "decision": {
      "label": "Decision",
      "title": "Lead with the OpenAI-Compatible Server",
      "copy": "Lead with the OpenAI-compatible server today and line up the native LM-Eval integration as a fast-follow once the APIs mature.",
      "widget": {
        "label": "Bonus",
        "copy": "This path gives MaxText a reusable OpenAI-format server that other teams can call for batch inference long after LM-Eval runs finish."
      }
    }
  },
  "slide6": {
    "kicker": "API Surface",
    "title": "The OpenAI-Compatible API",
    "lede": "MaxText now presents the exact completions and chat contracts that LM-Eval-Harness expects, so benchmark jobs plug in without custom glue.",
    "tabs": {
      "completions": {
        "label": "/v1/completions",
        "spotlight": "Legacy"
      },
      "chat": {
        "label": "/v1/chat/completions",
        "spotlight": "Chat"
      }
    },
    "labels": {
      "request": "Request",
      "response": "Response"
    },
    "completions": {
      "heading": "Text completions endpoint",
      "description": "Classic completion contract used by LM-Eval's generative tasks and rouge-based scorers.",
      "requestTitle": "Sample curl request",
      "requestAria": "Example curl command calling the /v1/completions endpoint",
      "request": "curl -X POST http://localhost:8000/v1/completions \\\n+  -H \"Content-Type: application/json\" \\\n+  -d '{\n    \"model\": \"maxtext-bison\",\n    \"prompt\": \"Who was the chief engineer of the Eiffel Tower?\",\n    \"max_tokens\": 64,\n    \"temperature\": 0.2,\n    \"top_p\": 0.9,\n    \"n\": 1,\n    \"stream\": false\n  }'",
      "responseTitle": "Response payload",
      "responseAria": "JSON payload returned from the /v1/completions endpoint",
      "response": "{\n  \"id\": \"cmpl-abc123\",\n  \"object\": \"text_completion\",\n  \"created\": 1725489273,\n  \"model\": \"maxtext-bison\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"text\": \"Gustave Eiffel's engineering firm led the design and construction of the tower.\",\n      \"finish_reason\": \"stop\",\n      \"logprobs\": null\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 14,\n    \"completion_tokens\": 22,\n    \"total_tokens\": 36\n  }\n}"
    },
    "chat": {
      "heading": "Chat completions endpoint",
      "description": "Chat-style contract for multi-turn prompts and judge-style evaluators.",
      "requestTitle": "Sample curl request",
      "requestAria": "Example curl command calling the /v1/chat/completions endpoint",
      "request": "curl -X POST http://localhost:8000/v1/chat/completions \\\n+  -H \"Content-Type: application/json\" \\\n+  -d '{\n    \"model\": \"maxtext-bison\",\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful evaluation assistant.\"\n      },\n      {\n        \"role\": \"user\",\n        \"content\": \"Score the factuality of the following answer...\"\n      }\n    ],\n    \"max_tokens\": 64,\n    \"temperature\": 0.0,\n    \"n\": 1,\n    \"stream\": false\n  }'",
      "responseTitle": "Response payload",
      "responseAria": "JSON payload returned from the /v1/chat/completions endpoint",
      "response": "{\n  \"id\": \"chatcmpl-xyz789\",\n  \"object\": \"chat.completion\",\n  \"created\": 1725489331,\n  \"model\": \"maxtext-bison\",\n  \"choices\": [\n    {\n      \"index\": 0,\n      \"message\": {\n        \"role\": \"assistant\",\n        \"content\": \"Factuality: 4/5. The answer cites Gustave Eiffel but omits Maurice Koechlin.\"\n      },\n      \"finish_reason\": \"stop\"\n    }\n  ],\n  \"usage\": {\n    \"prompt_tokens\": 52,\n    \"completion_tokens\": 24,\n    \"total_tokens\": 76\n  }\n}"
    }
  },
  "labels": {
    "thumbnails": "Slide previews"
  },
  "locales": {
    "en": {
      "label": "English",
      "nativeLabel": "English",
      "direction": "ltr"
    }
  }
}
