{
  "presentation": {
    "title": "Qwen3-Next Bench - What You Want To Know",
    "description": "Qwen3-Next baseline metrics and deployment evaluation",
    "author": "AutoGenerated"
  },
  "slide1": {
    "title": {
      "prefix": "Qwen3-",
      "highlight": "Next"
    },
    "subtitle": "Has the performance really made a leap?",
    "imageAlt": "Benchmark overview for Qwen3-Next",
    "keywords": {
      "gptAlternative": "A drop-in replacement for GPT-OSS-120B?",
      "latency": "First-token latency with a 256K context?",
      "memory": "How much VRAM is required?",
      "identify": "Which curve belongs to Qwen3-Next?",
      "deployment": "How do I deploy it locally?"
    }
  },
  "slide2": {
    "title": "Agenda",
    "items": {
      "baseline": "Baseline numbers",
      "comparison": "Qwen3-Next vs GPT-OSS-120B vs Qwen3-30B",
      "deployment": "Local deployment pitfalls"
    }
  },
  "slide3": {
    "title": "Benchmark Environment",
    "tabs": {
      "hardware": "Hardware",
      "framework": "Inference stack",
      "metrics": "Metrics",
      "models": "Models",
      "command": "Command"
    },
    "hardware": {
      "gpu": "4090 x 2, 48G",
      "cpu": "Dual Xeon Gen 4"
    },
    "frameworks": {
      "vllm": "VLLM",
      "tensorParallel": "Tensor parallel: 2",
      "mtp": "MTP: OFF",
      "context": "Context: 256K"
    },
    "metrics": {
      "baseline": "Baseline throughput",
      "speed": "Generation speed under different contexts",
      "latency": "First-token latency across context lengths"
    },
    "models": {
      "qwenNext": {
        "title": "Qwen3-Next-80B-A3B-Instruct",
        "subtitle": "(AWQ)"
      },
      "gptOss": {
        "title": "GPT-OSS-120B",
        "subtitle": "(MXFP4)"
      },
      "qwen30b": {
        "title": "Qwen3-30B-Instruct",
        "subtitle": "(AWQ)"
      }
    },
    "metricsCaption": "Context window grows from 512 up to 256K; results compared with GPT-OSS-120B and Qwen3-30B."
  },
  "slide4": {
    "title": "Baseline Performance",
    "cards": {
      "singleRequest": "Single request",
      "kvCache": "Effective context",
      "lowConcurrency": "Low concurrency, high throughput<br/>(8k input, 1k output, 16 concurrent)",
      "highConcurrency": "High concurrency, light output<br/>(1k input, 256 output, 256 concurrent)"
    }
  },
  "slide5": {
    "title": "Guess Which Curve Is Qwen3-Next",
    "iframeTitle": "Visualized benchmark comparison"
  },
  "slide6": {
    "title": "Deploy Without Pitfalls",
    "cards": {
      "fp8": {
        "title": "FP8 Works",
        "subtitle": "~120 t/s, ~120k context, minimal dynamic quant loss"
      },
      "awq": {
        "title": "AWQ build",
        "subtitle": "Mostly community quantization; quality varies"
      },
      "tokenizer": {
        "title": "tokenizer_config.json",
        "subtitle": "Keep an eye on version and long-context config"
      },
      "singleGpu": {
        "title": "Single 48G GPU",
        "subtitle": "Not viable (use 2x4090 or larger memory)"
      },
      "llamaCpp": {
        "title": "llama.cpp",
        "subtitle": "No solution yet - still waiting"
      },
      "mtp": {
        "title": "4090 x 2",
        "subtitle": "Enabling MTP is slower; keep it OFF"
      }
    }
  },
  "labels": {
    "thumbnails": "Slide previews"
  },
  "locales": {
    "en": {
      "label": "English",
      "nativeLabel": "English"
    },
    "zh-Hans": {
      "label": "Chinese (Simplified)",
      "nativeLabel": "简体中文"
    }
  }
}
